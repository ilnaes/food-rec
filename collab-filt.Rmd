```{r, include = FALSE}
knitr::opts_chunk$set(warning = FALSE)
```

```{r, message=FALSE}
library(tidyverse)
library(janitor)
library(stringi)
library(Matrix)
library(rsample)

source("ingredients-rec/ingr.rec.R")
source("prep.R")
```

```{r, message=FALSE}
df <- read_csv("outputs/facts.csv") %>%
  clean_names()
ingredients <- read_csv("outputs/ingredients.csv") %>%
  clean_names()
counts <- ingredients %>%
  count(id)
```

We define a metric, which is the average number of missing ingredients we end up recommending.

```{r}
metric <- function(truth, pred) {
  truth %>%
    map_lgl(~ . %in% pred) %>%
    mean()
}
```

The following function tests one recipe.

```{r}
test_row <- function(model, row, pow, n = 2) {
  shuf <- row %>%
    {
      \(x) x[as.logical(x)]
    }() %>%
    names() %>%
    sample()

  y <- shuf[1:n]
  x <- shuf[n + 1:length(shuf)]

  pred <- predict(model, x, pow)$name

  metric(y, pred)
}
```

This function will create a model for a particular rsmaple fold and test every recipe in the training part of the fold.

```{r}
test_fold <- function(fold, min_num = 2, pows = 0.5) {
  # make reproducible
  set.seed(2021)

  model <- fold %>%
    analysis() %>%
    left_join(ingredients, by = "id") %>%
    prep(min_num = min_num) %>%
    create_sim()

  test <- fold %>%
    assessment() %>%
    left_join(counts, by = "id") %>%
    filter(n >= 4) %>%
    select(-n) %>%
    left_join(ingredients, by = "id") %>%
    prep()

  tibble(
    min_num = min_num,
    pows = pows,
    metric = pows %>%
      map_dbl(~ apply(test, 1, \(x) test_row(model, x, .)) %>% mean())
  )
}
```

### Cross validation

```{r}
set.seed(2021)

split <- df %>%
  initial_split()

train <- training(split)
test <- testing(split)

folds <- vfold_cv(train, v = 5)
```

For each split, we create min_num from 2 to 5 and powers from 0.1 to 1 and cross validate.

```{r}
g <- expand.grid(2:5, 1:5)

res <- map2(
  g$Var1, g$Var2,
  ~ test_fold(folds$splits[[.y]],
    min_num = .x,
    pows = c(0.1, 0.25, 0.5, 1)
  ) %>%
    mutate(split = .y)
) %>%
  bind_rows()

res %>% 
  mutate(min_num = as_factor(min_num)) %>% 
  group_by(min_num, pows) %>%
  summarize(metric = mean(metric)) %>% 
  ggplot(aes(pows, metric, color = min_num)) +
  geom_line() +
  geom_point()
```

Looks like power of 0.25 and min_num of 2 works best.

### Run model on test split to get final metric

```{r}
model <- train %>%
  left_join(ingredients, on = "id") %>% 
  prep(min_num = 2) %>%
  create_sim()

test %>% 
  left_join(ingredients, on = "id") %>% 
  prep() %>% 
  apply(1, \(x) test_row(model, x, 0.25)) %>% 
  mean()
```

### Train model on all data to use for deployment

```{r}
ingredients %>% 
  prep(min_num = 2) %>% 
  create_sim() %>% 
  saveRDS("ingredients-rec/sim.RDS")
```
