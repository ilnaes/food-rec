```{r, include = FALSE}
knitr::opts_chunk$set(warning = FALSE)
```

```{r, message=FALSE}
library(tidyverse)
library(tidytext)
library(lubridate)

source("prep.R")
```

```{r, message=FALSE}
df <- read_csv("outputs/facts.csv") %>%
  mutate(year = year(date))

ingredients <- read_csv("outputs/ingredients.csv")
```

### Yearly numbers

Let's see how the number of recipes have been by year.

```{r}
df %>%
  count(year) %>%
  arrange(year)

df %>%
  count(year) %>%
  ggplot(aes(year, n)) +
  geom_line()

df %>%
  count(year) %>%
  mutate(n = cumsum(n)) %>%
  ggplot(aes(year, n)) +
  geom_line()
```

There weren't too many before 2004, a huge spike in 2004, and then a slow taper afterwards.  They probably did some backend retooling which re-dated all the articles before 2004 to 2004.  It is strange that the number of articles has been steadily decreasing.

Let's also look at the ratings.  Based on what we just saw, we will focus only after 2004.

```{r}
df %>%
  filter(year > 2004) %>%
  group_by(year) %>%
  summarize(rating = mean(rating, na.rm = TRUE)) %>%
  ggplot(aes(year, rating)) +
  geom_line()
```

Well the average rating has been going up, so it seems like Epicurious has been focusing on quality over quantity.

There was a serious spike in 2021, which is interesting.  The data was scraped in November, so there was enough data that it probably wasn't just chance.  Perhaps this was pandemic related?

### Authors

Who are the most prolific authors?

```{r}
unnest_authors <- df %>%
  unnest_tokens(author, author, token = str_split, pattern = ";")

unnest_authors %>%
  filter(!is.na(author)) %>%
  count(author) %>%
  arrange(-n)
```

Bunch of Bon Appetit people, not surprising given that Conde Nast owns both.  Let's look at when they were prolific.

```{r}
df %>%
  filter(!is.na(author)) %>%
  add_count(author) %>%
  filter(n >= 169) %>% # get top 10
  count(author, year) %>%
  ggplot(aes(year, n)) +
  geom_line() +
  facet_wrap(~author, scales = "free_y")
```

This all makes sense.  BA published as a team until around 2016, which is also when some of the BA people started showing up.  They were probably pushing the personalities more.  Also note that a lot of the BA people stopped showing up around last year, which was when the [blow up](https://www.eater.com/2020/8/6/21357341/priya-krishna-rick-martinez-sohla-el-waylly-resign-bon-appetit-test-kitchen-videos) happened.

We also look at the most prolific authors by year.

```{r}
unnest_authors %>%
  filter(year > 2004, !is.na(author)) %>%
  count(year, author) %>%
  group_by(year) %>%
  slice_max(n, n = 5)
```

### Ingredients

We now analyze the ingredients of recipes.  We first expand the table by each ingredient in the recipe.

```{r}
ingr <- df %>%
  left_join(ingredients, by = "id") %>%
  prep(to_mat = FALSE)

ingr
```

Base is the cleaned ingredient name.  Let's count how times each ingredients have shown up.  What are the most popular ingredients?

```{r}
ingr %>%
  count(base) %>%
  arrange(-n) %>%
  head(100)
```

What is the distribution of the number of ingredients per recipe like?

```{r}
ingr %>%
  count(id) %>%
  ggplot(aes(n)) +
  geom_histogram()
```

We will look at how ingredients change under two divisions: time and rating.  Let's first look at time.  We will split time into two parts: early <= 2004 (due to the timestamp in the data) and late >= 2014 (when Epicurious and BA were combined in a digital platform).  We want to calculate log odds, so we will ignore the inbetween.

```{r}
log_odds <- function(df, field, a, b) {
  df %>%
    count(base, {{ field }}) %>%
    group_by({{ field }}) %>%
    mutate(prob = (n + 1) / (sum(n) + 1)) %>%
    pivot_wider(base, names_from = {{ field }}, values_from = prob, values_fill = 0) %>%
    filter({{ a }} > 0, {{ b }} > 0) %>%
    mutate(log_odds = log({{ a }}, base = 2) - log({{ b }}, base = 2))
}
```

```{r}
ingr %>%
  mutate(period = case_when(
    year <= 2004 ~ "Early",
    year >= 2014 ~ "Late",
    TRUE ~ "Middle"
  )) %>%
  filter(period != "Middle") %>%
  log_odds(period, Late, Early) %>%
  {
    \(x) bind_rows(slice_max(., log_odds, n = 10), slice_min(., log_odds, n = 10))
  }() %>%
  mutate(base = fct_reorder(base, log_odds)) %>%
  ggplot(aes(log_odds, base)) +
  geom_col(aes(fill = log_odds > 0)) +
  theme(legend.position = "none") +
  labs(x = "Log odds of appearing in later recipes", y = "Ingredient")
```

We also look at the difference between the top and bottom quartiles.  Recipes of the bottom quartile tend to feature basic ingredients like pastas and legumes whereas the top rated recipes have more exotic ingredients like ramps, cotija, and grappa.

```{r}
quantile(df$rating, probs = c(0.25, 0.75), na.rm = TRUE)

ingr %>%
  mutate(group = case_when(
    rating <= 3 ~ "Lo",
    rating >= 3.66 ~ "Hi",
    TRUE ~ "Middle"
  )) %>%
  filter(group != "Middle") %>%
  log_odds(group, Hi, Lo) %>%
  {
    \(x) bind_rows(slice_max(., log_odds, n = 10), slice_min(., log_odds, n = 10))
  }() %>%
  mutate(base = fct_reorder(base, log_odds)) %>%
  ggplot(aes(log_odds, base)) +
  geom_col(aes(fill = log_odds > 0)) +
  theme(legend.position = "none") +
  labs(x = "Log odds of appearing in highly rated recipes", y = "Ingredient")
```

### Similarities

We now look at the similarity matrix that we will use in the recommender.

```{r}
source("ingredients-rec/ingr.rec.R")

sim <- ingr %>%
  prep(min_num = 10) %>%
  create_sim()
```

We will pivot the similarity table into long form.

```{r}
sims <- sim$sim %>%
  as_tibble() %>%
  mutate(ingr1 = colnames(.)) %>%
  pivot_longer(!ingr1, names_to = "ingr2", values_to = "val") %>%
  filter(ingr1 < ingr2)

sims
```

Let's look at a histogram of the similarities.

```{r}
sims %>%
  ggplot(aes(val)) +
  stat_ecdf()
```

Over 80% of pairs have no similarity value and most values are under 0.2.

We now look at a graph where we connect ingredients with particularly high similarty.

```{r, message=FALSE}
library(igraph)
library(ggraph)

set.seed(2022) # reproducibility

sims %>%
  filter(val > 0.22) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), size = 3, vjust = 1, hjust = 1)
```

We see some natural connected components like baking, asian stir fry, cocktail, and gluten free baking.

### Clustering

Finally, we will do clustering of the item vectors.  We first create the item vector matrix where the vectors are all unit.  This is code from the create_sim function.

```{r}
mat <- df %>%
  left_join(ingredients, by = "id") %>%
  prep(min_num = 5)

mat <- t(mat / sqrt(rowSums(mat)))
mat <- mat / sqrt(rowSums(mat * mat))
```

We now run kmeans to cluster the vectors.  Note that Euclidean distance between unit vectors corresponds well with the cosine of the angle by law of cosines.

```{r}
set.seed(2021) # reproducibility

res <- kmeans(mat, centers = 10, nstart = 10)

tibble(
  ingredient = rownames(mat),
  cluster = res$cluster
) %>%
  group_by(cluster) %>%
  slice_head(n = 5) %>%
  print(n = 50)
```

Some reasonable clusters.  Cluster 5 is obviously cocktails, 7 is probably sweet baked goods, 9 is more generic baked goods, and 10 is Asian cuisine.  The other clusters also have themes, but less obvious to interpretation.  It seems there is definitely information in the distance between item vectors.
