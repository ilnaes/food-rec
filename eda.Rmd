```{r}
library(tidyverse)
library(tidytext)
library(lubridate)

source("prep.R")
```

```{r}
df <- read_csv("outputs/facts.csv") %>%
  mutate(year = year(date))

ingredients <- read_csv("outputs/ingredients.csv")
```

# Quality and quantity per year

Let's see how the number of recipes have been by year.

```{r}
df %>%
  count(year) %>%
  arrange(year)

df %>%
  count(year) %>%
  ggplot(aes(year, n)) +
  geom_line()

df %>%
  count(year) %>%
  mutate(n = cumsum(n)) %>%
  ggplot(aes(year, n)) +
  geom_line()
```

There weren't too many before 2004, a huge spike in 2004, and then a slow taper afterwards.  They probably did some backend retooling which re-dated all the articles before 2004 to 2004.  It is strange that the number of articles has been steadily decreasing.

Let's also look at the ratings.  Based on what we just saw, we will focus only after 2004.

```{r}
df %>%
  filter(year > 2004) %>%
  group_by(year) %>%
  summarize(rating = mean(rating, na.rm = TRUE)) %>%
  ggplot(aes(year, rating)) +
  geom_line()
```

Well the average rating has been going up, so it seems like Epicurious has been focusing on quality over quantity.

There was a serious spike in 2021, which is interesting.  The data was scraped in November, so there was enough data that it probably wasn't just chance.

# Authors

What are the most prolific authors?

```{r}
unnest_authors <- df %>%
  unnest_tokens(author, author, token = str_split, pattern = ";")

unnest_authors %>%
  filter(!is.na(author)) %>%
  count(author) %>%
  arrange(-n)
```

Bunch of Bon Appetit people, not surprising given that Conde Nast owns both.  Let's look at when they were prolific.

```{r}
df %>%
  filter(!is.na(author)) %>%
  add_count(author) %>%
  filter(n >= 169) %>% # get top 10
  count(author, year) %>%
  ggplot(aes(year, n)) +
  geom_line() +
  facet_wrap(~author, scales = "free_y")
```

This all makes sense.  BA published as a team until around 2016, which is also when some of the BA people started showing up.  They were probably pushing the personalities more.  Also note that a lot of the BA people stopped showing up around last year, which was when the [blow up](https://www.eater.com/2020/8/6/21357341/priya-krishna-rick-martinez-sohla-el-waylly-resign-bon-appetit-test-kitchen-videos) happened.

```{r}
unnest_authors %>%
  count(id) %>%
  count(n) %>%
  arrange(n)
```


# Tags

Let's see what tags have been popular and what have been rated well

```{r}
df %>%
  unnest_tokens(tag, tags, token = str_split, pattern = ";") %>%
  group_by(tag) %>%
  summarize(
    n = n(),
    rating = mean(rating, na.rm = TRUE)
  ) %>%
  arrange(-n)
```

```{r}
df %>%
  unnest_tokens(tag, tags, token = str_split, pattern = ";") %>%
  add_count(tag) %>%
  filter(n > 9333, tag != "web", year > 2014) %>%
  count(tag, year) %>%
  ggplot(aes(year, n)) +
  geom_line() +
  facet_wrap(~tag, scales = "free_y")
```

# Ingredients

```{r}
ingr <- df %>%
  left_join(ingredients, by = "id") %>%
  prep(to_mat = FALSE)

ingr
```

```{r}
ingr %>%
  count(base) %>%
  arrange(-n) %>%
  head(500) %>%
  View()
```

We want to get a tf-idf of the ingredients.

```{r}
tf_idf <- function(df, field) {
  n_docs <- df %>%
    distinct({{ field }}) %>%
    nrow()

  tf <- df %>%
    group_by(base, {{ field }}) %>%
    summarize(n = n()) %>%
    ungroup(base) %>%
    mutate(tf = n / sum(n)) %>%
    ungroup()

  idf <- df %>%
    distinct(base, {{ field }}) %>%
    group_by(base) %>%
    count() %>%
    mutate(idf = log(n_docs) - log(n)) %>%
    select(-n) %>%
    ungroup()

  tf %>%
    left_join(idf, on = "base") %>%
    mutate(tf_idf = tf * idf)
}
```



```{r}
top_authors <- df %>%
  unnest_tokens(author, author, token = str_split, pattern = ";") %>%
  count(author) %>%
  filter(!is.na(author), author != "the bon appÃ©tit test kitchen") %>%
  arrange(-n) %>%
  head(20)

top_authors %>%
  left_join(df %>% unnest_tokens(author, author, token = str_split, patter = ";"),
    on = "author"
  ) %>% 
  select(-n) %>%
  left_join(ingredients %>% select(-author), by = "id") %>% 
  tf_idf(author) %>% 
  group_by(author) %>% 
  slice_max(tf_idf, n = 10) %>% 
  View()
```





```{r}
counts <- df %>%
  left_join(ingredients, on = "id") %>%
  prep(to_mat = FALSE) %>%
  add_count(id)

counts %>%
  distinct(id, n) %>%
  count(n) %>%
  arrange(n) %>%
  mutate(p = cumsum(nn) / sum(nn))

counts %>%
  filter(n > 100) %>%
  View()

counts %>%
  distinct(id, n) %>%
  ggplot(aes(n)) +
  geom_histogram()

counts %>%
  distinct(id, n) %>%
  filter(n > 30) %>%
  View()
```


```{r}
mat <- df %>%
  left_join(ingredients, by = "id") %>%
  add_count(base) %>%
  filter(n >= 5) %>%
  distinct(id, base) %>%
  mutate(n = 1) %>%
  pivot_wider(id, names_from = base, values_from = n, values_fill = 0) %>%
  select(-id) %>%
  as.matrix()

mat <- mat / sqrt(rowSums(mat * mat))
```

```{r}
res <- 2:15 %>%
  map(\(x) kmeans(mat, centers = x, nstart = 10))

saveRDS(res, "kms.RDS")
```

```{r}
res <- readRDS("kms.RDS")

tibble(x = 5:20, y = map_dbl(res, ~ .$tot.withinss)) %>%
  ggplot(aes(x = x, y = y)) +
  geom_line()

tibble(x = res[[15]]$cluster) %>%
  count(x)

df %>%
  left_join(ingredients, by = "id") %>%
  add_count(base) %>%
  filter(n >= 5) %>%
  distinct(id, name, base) %>%
  mutate(n = 1) %>%
  pivot_wider(c("id", "name"), names_from = base, values_from = n, values_fill = 0) %>%
  select(name) %>%
  mutate(cluster = res[[15]]$cluster) %>%
  group_by(cluster) %>%
  slice_head(n = 10) %>%
  View()
```






```{r}
library(stopwords)

good_bad <- df %>%
  unnest_tokens(word, hed) %>%
  anti_join(stop_words, by = "word") %>%
  group_by(word) %>%
  mutate(n = n()) %>%
  ungroup() %>%
  mutate(rating = case_when(
    aggregate_rating < 2.95 ~ "bad",
    aggregate_rating > 3.67 ~ "good",
    TRUE ~ "neutral",
  )) %>%
  filter(rating != "neutral")

log_odds <- good_bad %>%
  group_by(rating, word) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  pivot_wider(names_from = rating, values_from = n) %>%
  replace_na(list("good" = 0, "bad" = 0)) %>%
  mutate(
    good_ratio = (good + 1) / (sum(good) + 1),
    bad_ratio = (bad + 1) / (sum(bad) + 1),
    log_ratio = log2(good_ratio / bad_ratio)
  )

log_odds %>%
  filter(good + bad >= 10) %>%
  arrange(-abs(log_ratio)) %>%
  head(30) %>%
  mutate(word = fct_reorder(word, log_ratio)) %>%
  ggplot(aes(log_ratio, word)) +
  geom_col(aes(fill = log_ratio > 0))
```

People tend to rate exciting meals like meats and alcohol dishes well but basic dishes like pastas and starches low.

```{r}
good_bad <- df %>%
  unnest_tokens(word, hed) %>%
  anti_join(stop_words, by = "word") %>%
  group_by(word) %>%
  mutate(n = n()) %>%
  ungroup() %>%
  mutate(period = case_when(
    year <= 2004 ~ "early",
    year >= 2014 ~ "late",
    TRUE ~ "neutral",
  )) %>%
  filter(period != "neutral")

log_odds <- good_bad %>%
  group_by(period, word) %>%
  summarize(n = n()) %>%
  ungroup() %>%
  pivot_wider(names_from = period, values_from = n) %>%
  replace_na(list("early" = 0, "late" = 0)) %>%
  mutate(
    early_ratio = (early + 1) / (sum(early) + 1),
    late_ratio = (late + 1) / (sum(late) + 1),
    log_ratio = log2(late_ratio / early_ratio)
  )

log_odds %>%
  filter(early + late >= 10) %>%
  arrange(-abs(log_ratio)) %>%
  head(30) %>%
  mutate(word = fct_reorder(word, log_ratio)) %>%
  ggplot(aes(log_ratio, word)) +
  geom_col(aes(fill = log_ratio > 0))

log_odds %>%
  filter(early + late >= 10) %>%
  arrange(log_ratio)
```

```{r}
df %>%
  count(str_detect(dek, "^[Ee]ditor"))

df %>%
  filter(!str_detect(dek, "^[Ee]ditor")) %>%
  select(hed, dek) %>%
  View()
```

```{r}
write_lines(stringi::stri_trans_general(ingredients$ingredients, "latin-ascii"), "ingr.txt")
```

```{r}
library(stringi)

words <- df %>%
  mutate(
    new = hed %>%
      stri_trans_general(id = "Latin-ASCII") %>%
      str_remove_all("\\(.*?\\)"),
    new2 = new
  ) %>%
  unnest_tokens(word, new)

words %>%
  group_by(id) %>%
  count() %>%
  ggplot(aes(n)) +
  geom_histogram()

words %>%
  group_by(id) %>%
  count() %>%
  arrange(-n)

words %>%
  distinct(word) %>%
  write_csv("data/vocab.csv")
```

```{r}
deks_counts <- df %>%
  unnest_tokens(dek, dek) %>%
  filter(!is.na(dek)) %>%
  group_by(id) %>%
  summarize(dek = n())

hed_counts <- df %>%
  unnest_tokens(hed, hed) %>%
  filter(!is.na(hed)) %>%
  group_by(id) %>%
  summarize(hed = n())

counts <- hed_counts %>%
  left_join(deks_counts, by = "id") %>%
  mutate(dek = replace_na(dek, 0))

counts %>%
  ggplot(aes(hed, dek)) +
  geom_point() +
  geom_smooth(method = "lm")

lm(dek ~ hed, counts) %>%
  summary()

conda_install(envname = "ret", packages = "nltk")
conda_list()
```


```{r}
library(stopwords)

df2 <- read_csv("data/RAW_recipes.csv") %>%
  filter(!is.na(description)) %>%
  mutate(
    description = str_replace_all(description, "\r\n", " "),
    description = str_replace_all(description, "\\.(\\W+)", " punct \\1"),
    description = str_replace_all(description, "[\\?!]", " punct "),
    description = str_replace_all(description, ",", " commapunct "),
    description = str_replace_all(description, " \\s+", " ")
  )
```

```{r}
write_lines(x = df2 %>% pull(description), file = "test.txt")
```


```{r}
library(word2vec)
model <- read.word2vec("glove_w2v.bin")
```

```{r}
mat <- matrix(ncol = 50, nrow = nrow(df2))
rownames(mat) <- df2$name

for (i in 1:nrow(df2)) {
  if (is.na(df2$description[[i]])) {
    next
  }
  tokens <- hunspell::hunspell_parse(df2$description[[i]])[[1]] %>%
    Filter(f = \(x) !(x %in% stop_words$word))

  if (length(tokens) > 0) {
    vec <- predict(model, newdata = tokens, type = "embedding") %>%
      rbind() %>%
      colMeans(na.rm = TRUE)
    mat[i, ] <- vec
  }
}
```

```{r}
res <- kmeans(t(apply(mat[complete.cases(mat), ], 1, \(x) x / sqrt(sum(x^2)))), centers = 9)
```


```{r}
tibble(
  cluster = res$cluster,
  name = rownames(mat[complete.cases(mat), ]),
  # description = df2$description[complete.cases(mat)],
) %>%
  arrange(cluster) %>%
  filter(cluster == 7) %>%
  sample_n(10)
```
